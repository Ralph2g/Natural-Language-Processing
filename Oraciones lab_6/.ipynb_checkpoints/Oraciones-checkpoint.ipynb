{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos en un Pk los objetos\n",
    "def serializeObject(obj, fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "#desconvertimos los pkl\n",
    "def deserializeObject(fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_string(fname):\n",
    "    '''Receives an html file with a Spanish text, deletes html tags.\n",
    "    Returns text as a string.'''\n",
    "    \n",
    "    from bs4   import BeautifulSoup\n",
    "    \n",
    "    f=open(fname, encoding='utf-8')\n",
    "    text_string=f.read()\n",
    "    f.close()\n",
    "\n",
    "    soup = BeautifulSoup(text_string, 'lxml')\n",
    "    text_string = soup.get_text()\n",
    "        \n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text_string(text_string, fname):\n",
    "    '''Writes a text string into a text file'''\n",
    "    \n",
    "    f = open(fname, \"w\", encoding='utf-8')\n",
    "    f.write(text_string) \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_txt_string(fname):\n",
    "    f=open(fname, encoding='utf-8')\n",
    "    text_string=f.read()\n",
    "    f.close()\n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(string):\n",
    "    #dirección del archivo a obtener oraciones:\n",
    "    root = './'\n",
    "    text = PlaintextCorpusReader(root, 'text.txt')\n",
    "    return text.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(wordsWithTag, language='spanish'):\n",
    "    '''Receives a list of words and returns another list without stop words'''\n",
    "    return [ word for word in wordsWithTag if word not in stopwords.words(language) ]\n",
    "\n",
    "\n",
    "def clearTokens(tokensWithTag):\n",
    "    '''Receives a list of  with tag and returns another list with the same tokens but only with letters'''\n",
    "    result = []\n",
    "    for token in tokensWithTag:\n",
    "        clearToken = \"\"\n",
    "        for c in token:\n",
    "            if re.match(r'[a-záéíóúñüA-ZÁÉÍÓÚÑÜ]', c):\n",
    "                clearToken += c\n",
    "        if len(clearToken) > 0:\n",
    "            result.append(clearToken)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(sents):\n",
    "    #Taggeamos el texto\n",
    "    sentences_tagged = []\n",
    "    #spanish_tagger = deserializeObject(\"tagger.pkl\")#agarramos nuestro tagger entrenado\n",
    "    for s in sentences:\n",
    "        #s_tagged = spanish_tagger.tag(s)#Taggeamos los tokens\n",
    "        s_tagged = [(it.lower()) for it in s]#Pasamos a minusculas    \n",
    "        clearedTokens = removeStopWords(clearTokens(s_tagged))\n",
    "        sentences_tagged.append(clearedTokens)#Agregamos las oraciones taggeadas\n",
    "    return sentences_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(sentences):\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            words.add(word)\n",
    "    serializeObject(words,'vocabulary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_sents_with_word(w):\n",
    "    sents = deserializeObject('pro_sents.pkl')\n",
    "    count_w = 0\n",
    "    for sent in sents:\n",
    "        if w in sent:\n",
    "            count_w += 1\n",
    "    return count_w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(a,b):\n",
    "    if a == 0 or b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log2(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_sents_with_words(w1,w2):\n",
    "    sents = deserializeObject('pro_sents.pkl')\n",
    "    count_ww = 0\n",
    "    for sent in sents:\n",
    "        if w1 in sent and w2 in sent:\n",
    "            count_ww += 1\n",
    "    return count_ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #Cadena de texto\n",
    "    #fname='text.txt'#Cambiamos por txt despues de procesar htm------2\n",
    "    #text_string = get_text_string(fname)\n",
    "    #write_text_string(text_string, 'text.txt')\n",
    "    \n",
    "    #2.1 Obtenemos oraciones taggeadas\n",
    "    #text_string=get_txt_string(fname)#------Obtenemos la cadena del txt-----2\n",
    "    #Obenemos oraciones\n",
    "    #sentences = get_sentences(text_string)# Obtenemos las oraciones del texto -----2\n",
    "    #Tenemos que normalizar las oraciones\n",
    "    #pro_sentences = normalize(sentences)# Normalizamos cada oración del texto -----2\n",
    "    #serializeObject(pro_sentences,'pro_sents.pkl')#Lo guardamos para ahorrar procesamiento-----2\n",
    "    \n",
    "    #Parte 3\n",
    "    pro_sentences = deserializeObject('pro_sents.pkl')\n",
    "    #Creamos nuestro vocabulario\n",
    "    \n",
    "    #create_vocabulary(pro_sentences)\n",
    "    vocabulary=deserializeObject('vocabulary.pkl')\n",
    "    word = 'empresa'\n",
    "    list_entropy = []\n",
    "    \n",
    "    #calculating p_w1=1\n",
    "    p_w1_1 = ((number_sents_with_word(word))/len(pro_sentences))\n",
    "    p_w1_0 = (1-p_w1_1)\n",
    "    for w2 in vocabulary:\n",
    "        list_aux = [] #list that saves the word with the entropy\n",
    "        #base cases\n",
    "        p_w2_1 = ((number_sents_with_word(w2))/len(pro_sentences))\n",
    "        p_w2_0 = (1-p_w2_1)\n",
    "        p_w1_1_w2_1 = ((number_sents_with_words(word,w2))/len(pro_sentences))\n",
    "        #reflecting variables\n",
    "        p_w1_0_w2_1 = p_w2_1 - p_w1_1_w2_1\n",
    "        p_w1_1_w2_0 = p_w1_1 - p_w1_1_w2_1\n",
    "        p_w1_0_w2_0 = p_w2_0 - p_w1_1_w2_0\n",
    "        Entropy = (-1)* ( ( (p_w1_0_w2_0)*log2( (p_w1_0_w2_0),(p_w2_0) ) ) + ( (p_w1_1_w2_0)*log2( (p_w1_1_w2_0),(p_w2_0) ) ) + ( (p_w1_0_w2_1)*log2( ( p_w1_0_w2_1),(p_w2_1) ) ) + ( (p_w1_1_w2_1)*log2( (p_w1_1_w2_1),(p_w2_1) ) ) )\n",
    "        list_aux.append(w2)\n",
    "        list_aux.append(Entropy)\n",
    "        list_entropy.append(list_aux)\n",
    "    serializeObject(list_entropy,'entropy_w1_w2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
