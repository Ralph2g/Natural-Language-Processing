{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos los vectores X y Y definidos en clase\n",
    "def create_vector_x_y(m):\n",
    "    \n",
    "    start = 0\n",
    "    words = []\n",
    "    vec_y = []\n",
    "    for ix, x in enumerate(m):\n",
    "        if x == ',':\n",
    "            aux = m[(ix+1):(ix+5)]\n",
    "            \n",
    "            if( aux == 'spam'):\n",
    "                words.append(m[start:ix])\n",
    "                start = ix + 6## se suman dos mas por los saltos de linea\n",
    "                vec_y.append(1)\n",
    "                \n",
    "            if( (aux[0:3] == 'ham') ):\n",
    "                words.append(m[start:ix])\n",
    "                start = ix + 5 ## se suman dos mas por los saltos de linea\n",
    "                vec_y.append(0)\n",
    "    #Pasamos a minusculas y además tokenizamos\n",
    "    X_corpus = [line.lower() for line in words]\n",
    "    X_corpus = [nltk.word_tokenize(line) for line in X_corpus]\n",
    "    \n",
    "    #Vamos a taggear\n",
    "    X_pos_tagged  = []\n",
    "    for text in X_corpus:\n",
    "        pos_tagged_text = nltk.pos_tag(text)\n",
    "        X_pos_tagged.append(pos_tagged_text)\n",
    "    \n",
    "    print(\"\\nText Post tagged \\n\")\n",
    "    print(X_pos_tagged[500])\n",
    "    \n",
    "    #Vamos a lematizar\n",
    "    from nltk import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X_lemmatized = []\n",
    "    for text in X_pos_tagged:\n",
    "        lemmas = []\n",
    "        for token in text:\n",
    "            str1 = token[0]\n",
    "            str2 = token[1].lower()\n",
    "            try:\n",
    "                str2 = str2[0]\n",
    "                lemma = lemmatizer.lematize(str1,str2)\n",
    "                lemmas.append(lemma.lower())\n",
    "            except:\n",
    "                lemmas.append(str1.lower())\n",
    "        X_lemmatized.append(lemmas)\n",
    "    print(\"TEXT Lematized\\n\")\n",
    "    print(X_lemmatized[500])\n",
    "    \n",
    "    #making vocabulart\n",
    "    one_list = []\n",
    "    for text in X_lemmatized:\n",
    "        one_list = one_list + text\n",
    "    fd = nltk.FreqDist(one_list)\n",
    "    vocabulary = sorted(list(fd.keys()))\n",
    "    \n",
    "    print (\"\\n the vocabulary has %d words \\n\" %len(vocabulary))\n",
    "    print (vocabulary[:50])\n",
    "    \n",
    "    X_frequencies = []\n",
    "\n",
    "    for text in X_lemmatized:\n",
    "        vector = [1]\n",
    "        for voc in vocabulary:\n",
    "            vector.append(text.count(voc))\n",
    "        X_frequencies.append(vector)\n",
    "\n",
    "    #Vamos a mezcalar los datos para poder entrenar el algoritmo guardando su correspondecia\n",
    "    import random\n",
    "    mapIndexPosition = list(zip(X_frequencies,vec_y))\n",
    "    random.seed(30)\n",
    "    random.shuffle(mapIndexPosition)\n",
    "    X_frequencies, y = zip(*mapIndexPosition)\n",
    "    X_frequencies = list(X_frequencies)\n",
    "    y = list(y)\n",
    "\n",
    "        #split data into trainig set of test set\n",
    "    test_size = 0.2\n",
    "    split_index = int(len(X_frequencies) * test_size)\n",
    "    X_test = X_frequencies[:split_index]\n",
    "    y_test = y[:split_index]\n",
    "    X_train = X_frequencies[split_index:]\n",
    "    y_train = y[split_index:]\n",
    "    \n",
    "    print(\"X_test lenght:\",len(X_test))\n",
    "    print(\"Y_test lenght:\",len(y_test))\n",
    "    print(\"X_train lenght:\",len(X_train))\n",
    "    print(\"Y_train lenght:\",len(y_train))\n",
    "    \n",
    "    m_train = len(X_train) # number of examples in X_train\n",
    "    m_test =len(X_test) # number of examples in X_test\n",
    "    num_features = len(vocabulary) + 1\n",
    "    \n",
    "    #Ahora pasamos a la parte de calculo ya que tenemos nuestras matrices\n",
    "\n",
    "    import numpy as np\n",
    "    #convert 'X' an ' y' Python list\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    X_train = X_train.T\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = X_test.T\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    #print \n",
    "    y_train = y_train.reshape((m_train, 1))\n",
    "    y_test = y_test.reshape ((m_test, 1))\n",
    "    #################3\n",
    "    X = np.array(X_frequencies)\n",
    "    X = X.T # now in X,row features and columns are exampples\n",
    "    X_shape = X.shape\n",
    "    print ('The shape of numpy x is', X.shape)\n",
    "    y = np.array(y)\n",
    "    m = len(X_lemmatized) # number of text in the example\n",
    "    num_features = len(vocabulary)+1\n",
    "    y = y.reshape((m,1)) #cambiamos las dimensiones ya que shape muestra (1324,) ahora (1324,1)\n",
    "    print('\\n The shape of numpy y is',y.shape)#shape lo que muestra es \n",
    "\n",
    "    #Ahora se programa el descenso de gradeinte\n",
    "    theta = np.zeros((num_features,1))\n",
    "    #\n",
    "    alpha = 10.3\n",
    "    num_iter = 1000\n",
    "    \n",
    "    costs = []\n",
    "    print ('Gradient descend')\n",
    "    for i in range(num_iter):\n",
    "        #compute predictions --Primero son los calculos\n",
    "\n",
    "        z = np.dot(theta.T,X_train) # z is a row vector \n",
    "        z = z.T # now z is a column vector la T es de transposición\n",
    "\n",
    "        y_pred = 1/(1 + np.exp(-z))\n",
    "\n",
    "        assert(y_train.shape == y_pred.shape)\n",
    "\n",
    "        #se calucla el costo J(0)\n",
    "\n",
    "        a = np.multiply(y_train, np.log(y_pred))\n",
    "        b = np.multiply (1-y_train, np.log(1-y_pred))\n",
    "\n",
    "        cost = (-1/m_train)* np.sum(a + b)\n",
    "        costs.append(cost)\n",
    "        #print each 10 iteratiosn\n",
    "        if i%50 == 0:\n",
    "            print('Cost iteration ', i, 'is', cost)\n",
    "\n",
    "        d_theta = (1/m_train)*np.dot((y_pred - y_train).T, X_train.T)\n",
    "        d_theta = d_theta.T\n",
    "\n",
    "        # secaclula la derivaad parcial\n",
    "        theta = theta - alpha * d_theta #partial derivates with respect to theta\n",
    "        #luago se calcula la derivada de Theta\n",
    "    \n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    #costs = np.squeeze(d['costs'])\n",
    "    fig  = plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.savefig('costs')\n",
    "    #Now, we have to classify our examples\n",
    "    \n",
    "    z = np.dot(theta.T,X_train)\n",
    "    z = z.T\n",
    "    \n",
    "    y_train_pred = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    predictions_train = []\n",
    "    \n",
    "    for item in y_train_pred:\n",
    "        if item >= 0.5:\n",
    "            predictions_train.append(1)\n",
    "        else:\n",
    "            predictions_train.append(0)\n",
    "    \n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == 1:\n",
    "            if predictions_train[i] == 1:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        elif y_train[i] == 0:\n",
    "            if predictions_train[i] == 1:\n",
    "                false_pos += 1\n",
    "            else:\n",
    "                true_neg += 1\n",
    "    \n",
    "    print(\"\\nModel evaluation on the train set:\")\n",
    "    accuracy = (true_pos + true_neg)/m_train\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    F1 = (2 * precision * recall)/ (precision + recall)\n",
    "    \n",
    "    print(\"precision is: \",precision)\n",
    "    print(\"Recall is \", recall)\n",
    "    print(\"F1 is\", F1)\n",
    "    \n",
    "    z = np.dot(theta.T,X_test)\n",
    "    z = z.T\n",
    "    \n",
    "    y_test_pred = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for item in y_test_pred:\n",
    "        if item >= 0.5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    \n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] == 1:\n",
    "            if predictions[i] == 1:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        elif y_test[i] == 0:\n",
    "            if predictions[i] == 1:\n",
    "                false_pos += 1\n",
    "            else:\n",
    "                true_neg += 1\n",
    "    \n",
    "    print(\"\\nModel evaluation on the Test set:\")\n",
    "    accuracy = (true_pos + true_neg)/m_test\n",
    "    precision = true_pos/(true_pos + false_pos)\n",
    "    recall = true_pos/(true_pos + false_neg)\n",
    "    F1 = (2 * precision * recall)/ (precision + recall)\n",
    "    \n",
    "    print(\"precision is: \",precision)\n",
    "    print(\"Recall is \", recall)\n",
    "    print(\"F1 is\", F1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Post tagged \n",
      "\n",
      "[('juz', 'NN'), ('now', 'RB'), ('havent', 'VBZ'), ('woke', 'VBD'), ('up', 'RP'), ('so', 'RB'), ('a', 'DT'), ('bit', 'NN'), ('blur', 'JJ'), ('blur', 'NN'), ('...', ':'), ('can', 'MD'), ('?', '.'), ('dad', 'VB'), ('went', 'VBD'), ('out', 'RP'), ('liao', 'NN'), ('...', ':'), ('i', 'NN'), ('cant', 'VBP'), ('cum', 'NN'), ('now', 'RB'), ('oso', 'VBZ'), ('...', ':')]\n",
      "TEXT Lematized\n",
      "\n",
      "['juz', 'now', 'havent', 'woke', 'up', 'so', 'a', 'bit', 'blur', 'blur', '...', 'can', '?', 'dad', 'went', 'out', 'liao', '...', 'i', 'cant', 'cum', 'now', 'oso', '...']\n",
      "\n",
      " the vocabulary has 3750 words \n",
      "\n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'help\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '(', ')', '*****', '***********', '***************', '*****up', '+', '+123', '+447797706009', '+449071512431', '+cheer', '+get', '+std', '+£400', ',', '-', '-call', '-message', '-no', '-sounds', '-sub', '.', '...', '..please', '.terry', '//careers', '//www.urawinner.com', '/colour', '0', '008704050406', '0121', '01223585334', '0125698789', '02', '02/06/03', '02/09/03', '02072069400.']\n",
      "X_test lenght: 265\n",
      "Y_test lenght: 265\n",
      "X_train lenght: 1060\n",
      "Y_train lenght: 1060\n",
      "The shape of numpy x is (3751, 1325)\n",
      "\n",
      " The shape of numpy y is (1325, 1)\n",
      "Gradient descend\n",
      "Cost iteration  0 is 0.6931471805599453\n",
      "Cost iteration  50 is 0.012717403808053307\n",
      "Cost iteration  100 is 0.007122949347903471\n",
      "Cost iteration  150 is 0.004930835375694593\n",
      "Cost iteration  200 is 0.003767703704167678\n",
      "Cost iteration  250 is 0.003048188044490621\n",
      "Cost iteration  300 is 0.002559440214767506\n",
      "Cost iteration  350 is 0.0022058613874377858\n",
      "Cost iteration  400 is 0.0019382136572496831\n",
      "Cost iteration  450 is 0.0017285726337368658\n",
      "Cost iteration  500 is 0.0015599262804980793\n",
      "Cost iteration  550 is 0.0014213206969998905\n",
      "Cost iteration  600 is 0.0013053846065975527\n",
      "Cost iteration  650 is 0.0012069757048743914\n",
      "Cost iteration  700 is 0.0011223975132364477\n",
      "Cost iteration  750 is 0.0010489246845610991\n",
      "Cost iteration  800 is 0.000984503779016281\n",
      "Cost iteration  850 is 0.0009275582394880497\n",
      "Cost iteration  900 is 0.0008768575607542348\n",
      "Cost iteration  950 is 0.0008314272856810615\n",
      "\n",
      "Model evaluation on the train set:\n",
      "precision is:  1.0\n",
      "Recall is  1.0\n",
      "F1 is 1.0\n",
      "\n",
      "Model evaluation on the Test set:\n",
      "precision is:  0.971830985915493\n",
      "Recall is  0.9452054794520548\n",
      "F1 is 0.9583333333333334\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    import nltk\n",
    "    file = open(\"SMS_Spam_Corpus_big.txt\", \"r\")\n",
    "    mensaje = file.read()\n",
    "    create_vector_x_y(mensaje)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12312']\n",
      "123,\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "string = '123123123,1231,23123123'\n",
    "iz = 0\n",
    "ix = 5\n",
    "words =[]\n",
    "\n",
    "words.append(string[iz:ix])\n",
    "print(words)\n",
    "\n",
    "iz = ix + 1\n",
    "aux = string[(ix+1):(ix+5)]\n",
    "words.append(string[iz:])\n",
    "print(aux)\n",
    "if (aux[0:3] == '123' ):\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123123123', '1231', '23123123']\n"
     ]
    }
   ],
   "source": [
    "s = '123123123,1231,23123123,'\n",
    "start = 0\n",
    "words = []\n",
    "    \n",
    "for ix, x in enumerate(s):\n",
    "    if x == ',':\n",
    "        words.append(s[start:ix])\n",
    "        start = ix + 1\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = 500\n",
    "end_index = 504\n",
    "\n",
    "f = open(\"SMS_Spam_Corpus_big.txt\")\n",
    "lines = f.readlines()\n",
    "X_corpus = [line.lowe() for line in lines]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_frequencies = []\n",
    "\n",
    "for text in X_lemmatized:\n",
    "    vector = [1]\n",
    "    for voc in vocabulary:\n",
    "        vector.append(text.count(voc))\n",
    "        X_frequencies.append(vector)\n",
    "        \n",
    "#Vamos a mezcalar los datos para poder entrenar el algoritmo guardando su correspondecia\n",
    "import random\n",
    "mapIndexPosition = list(zip(X_frequencies,y))\n",
    "random.seed(30)\n",
    "random.suffle(mapIndexPosition)\n",
    "X_frequencies = list(X_frequencies)\n",
    "y = list(y)\n",
    "    \n",
    "    #split data into trainig set of test set\n",
    "test_size = 0.2\n",
    "split_index = int(len(X_frequencies) * test_size)\n",
    "X_test = X_frequencies[:split_index]\n",
    "y_test = y[:split_index]\n",
    "X_train = X_frequencies[split_index:]\n",
    "y_train = y[split_index:]\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "#Ahora pasamos a la parte de calculo ya que tenemos nuestras matrices\n",
    "\n",
    "import numpy as np\n",
    "#convert 'X' an ' y' Python list\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_train = X_train.T\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.T\n",
    "\n",
    "#print \n",
    "\n",
    "\n",
    "\n",
    "y_train = y_train.reshape((m_train,1))\n",
    "\n",
    "#################3\n",
    "X = np.array(X_frequencies)\n",
    "X = X.T # now in X,row features and columns are exampples\n",
    "X_shape = X.shape\n",
    "print ('The shape of numpy x is', X.shape)\n",
    "y = np.array(y)\n",
    "m = len(X_lemmatized) # number of text in the example\n",
    "num_features = len(vocabulary)+1\n",
    "y = y.reshape((m,1)) #cambiamos las dimensiones ya que shape muestra (1324,) ahora (1324,1)\n",
    "print('\\n The shape of numpy y is'y.shape)#shape lo que muestra es \n",
    "\n",
    "\n",
    "#Ahora se programa el descenso de gradeinte\n",
    "theta = np.zeros((num_features,1))\n",
    "#\n",
    "alpha = 0.3#10.3\n",
    "num_iter = 1000\n",
    "\n",
    "print ('Gradient descend')\n",
    "for i in range(num_iter):\n",
    "    #compute predictions --Primero son los calculos\n",
    "    \n",
    "    z = np.dot(theta.T,X) # z is a row vector \n",
    "    z = z.T # now z is a column vector la T es de transposición\n",
    "    \n",
    "    y_pred = 1/(1 + np.exp(-z))\n",
    "    \n",
    "    assert(y.shape == y_pred.shape)\n",
    "    \n",
    "    #se calucla el costo J(0)\n",
    "    \n",
    "    a = np.multiply(y, np.log(y_pred))\n",
    "    b = np.multiply (1-y, np.log(1-y_pred))\n",
    "    \n",
    "    cost = (-1/m)* np.sum(a + b)\n",
    "    \n",
    "    #print each 10 iteratiosn\n",
    "    if i%50 == 0:\n",
    "        print('Cost iteration ', i, 'is', cost)\n",
    "    \n",
    "    d_theta = (1/m)*np.dot((y_pred - y).T, X.T)\n",
    "    d_theta = d_theta.T\n",
    "    \n",
    "    # secaclula la derivaad parcial\n",
    "    theta = theta - alpha * d_theta #partial derivates with respect to theta\n",
    "    #luago se calcula la derivada de Theta\n",
    "\n",
    "\n",
    "    \n",
    "#instalar Sci-Kit learn\n",
    "#Todo esto esta en el libro de text data management en el capitulo 15\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
