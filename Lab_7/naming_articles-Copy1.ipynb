{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from prettytable import PrettyTable\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos en un Pk los objetos\n",
    "def serializeObject(obj, fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "#desconvertimos los pkl\n",
    "def deserializeObject(fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(fname):\n",
    "        \n",
    "    f=open(fname, encoding='utf-8')\n",
    "    text_string=f.read()\n",
    "    f.close()\n",
    "    list_articles = text_string.split('<h3>')\n",
    "    return list_articles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_string(articles):\n",
    "    '''Receives an html file with a Spanish text, deletes html tags.\n",
    "    Returns text as a string.'''\n",
    "    list_articles = []\n",
    "    for text_string in articles:\n",
    "        soup = BeautifulSoup(text_string, 'lxml')\n",
    "        text_string = soup.get_text()\n",
    "        list_articles.append(text_string)\n",
    "    return list_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagSentencesSpanishTagger(sentences):\n",
    "    sentences_tagged = []\n",
    "    spanish_tagger = deserializeObject(\"tagger.pkl\")#agarramos nuestro tagger entrenado\n",
    "    for s in sentences:\n",
    "        tokens = nltk.word_tokenize(s)#Tokenizamos los articulos\n",
    "        s_tagged = spanish_tagger.tag(tokens)#Taggeamos los tokens\n",
    "        s_tagged = [(it[0].lower(), it[1][0].lower()) for it in s_tagged]#Pasamos a minusculas\n",
    "        sentences_tagged.append(lemmatize(removeStopWords(clearTokens(s_tagged))))#Agregamos las oraciones taggeadas\n",
    "    return sentences_tagged\n",
    "#Sacamos las palabras de las oraciones de todas las sentencias\n",
    "def getWordsFromSentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            words.append(word)\n",
    "    return words\n",
    "#Limpiamos los tokens\n",
    "\n",
    "def removeStopWords(wordsWithTag, language='spanish'):\n",
    "    '''Receives a list of words and returns another list without stop words'''\n",
    "    return [ word for word in wordsWithTag if word[0] not in stopwords.words(language) ]\n",
    "\n",
    "\n",
    "def clearTokens(tokensWithTag):\n",
    "    '''Receives a list of  with tag and returns another list with the same tokens but only with letters'''\n",
    "    result = []\n",
    "    for token in tokensWithTag:\n",
    "        clearToken = \"\"\n",
    "        for c in token[0]:\n",
    "            if re.match(r'[a-záéíóúñüA-ZÁÉÍÓÚÑÜ]', c):\n",
    "                clearToken += c\n",
    "        if len(clearToken) > 0:\n",
    "            result.append((clearToken, token[1]))\n",
    "    return result\n",
    "\n",
    "def lemmatize(wordsTagged, fname=\"lemmas2.pkl\"):\n",
    "    lemmas = deserializeObject(fname)\n",
    "    wordsLemmatized = []\n",
    "    for word in wordsTagged:\n",
    "        if word in lemmas.keys():#Si la palabra la encuentra en el dic\n",
    "            wordsLemmatized.append((lemmas[word], word[1]))#Guardamos el lemma con su tag y la agregamos a las palabras lematizadas\n",
    "        else:\n",
    "            wordsLemmatized.append(word)#Si no la encuentra guarda la palabra en el vocabulario en el diccionario\n",
    "    return wordsLemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n",
      "+----+----------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "| N° |  ['gobierno', 'n']   |  ['política', 'n']  | ['internet', 'n']  |   ['méxico', 'n']    |   ['mercado', 'n']  |\n",
      "+----+----------------------+---------------------+--------------------+----------------------+---------------------+\n",
      "| 0  |          0           |          0          |         0          |          0           |          0          |\n",
      "| 1  |          0           |          0          |         0          |          0           |          0          |\n",
      "| 2  |         1.0          |         0.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 3  |         0.0          |         0.5         |        0.0         |         0.0          |         0.5         |\n",
      "| 4  |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 5  |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 6  |         1.0          |         0.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 7  |          0           |          0          |         0          |          0           |          0          |\n",
      "| 8  |          0           |          0          |         0          |          0           |          0          |\n",
      "| 9  |          0           |          0          |         0          |          0           |          0          |\n",
      "| 10 |  0.3333333333333333  |  0.3333333333333333 |        0.0         |  0.3333333333333333  |         0.0         |\n",
      "| 11 |         0.0          |         0.0         |        0.0         |         0.5          |         0.5         |\n",
      "| 12 |         1.0          |         0.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 13 |         0.75         |         0.0         |        0.0         |         0.25         |         0.0         |\n",
      "| 14 |         0.5          |         0.5         |        0.0         |         0.0          |         0.0         |\n",
      "| 15 |        0.125         |         0.25        |        0.0         |         0.0          |        0.625        |\n",
      "| 16 |  0.9473684210526315  | 0.05263157894736842 |        0.0         |         0.0          |         0.0         |\n",
      "| 17 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 18 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 19 |         0.0          |         1.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 20 | 0.17391304347826086  |  0.5217391304347826 |        0.0         | 0.30434782608695654  |         0.0         |\n",
      "| 21 |         0.0          |        0.1875       |        0.0         |        0.3125        |         0.5         |\n",
      "| 22 | 0.09090909090909091  |         0.0         |        0.0         |  0.7272727272727273  | 0.18181818181818182 |\n",
      "| 23 |         0.0          |         1.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 24 |  0.5714285714285714  |         0.0         |        0.0         | 0.14285714285714285  |  0.2857142857142857 |\n",
      "| 25 |         0.0          | 0.10526315789473684 |        0.0         |  0.3684210526315789  |  0.5263157894736842 |\n",
      "| 26 |         0.75         |         0.25        |        0.0         |         0.0          |         0.0         |\n",
      "| 27 |         0.0          |         0.25        |        0.0         |         0.0          |         0.75        |\n",
      "| 28 |         0.0          |         0.0         |        0.0         |         0.75         |         0.25        |\n",
      "| 29 |         0.0          |         0.0         |        0.0         |  0.6666666666666666  |  0.3333333333333333 |\n",
      "| 30 |         0.0          |         0.0         | 0.8333333333333334 | 0.16666666666666666  |         0.0         |\n",
      "| 31 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 32 |         0.0          |         0.0         |        0.0         |         0.5          |         0.5         |\n",
      "| 33 |         0.0          |         0.0         | 0.9090909090909091 | 0.09090909090909091  |         0.0         |\n",
      "| 34 | 0.043478260869565216 |         0.0         | 0.9130434782608695 | 0.043478260869565216 |         0.0         |\n",
      "| 35 |         0.0          |         0.0         |        0.0         |         0.5          |         0.5         |\n",
      "| 36 |         0.1          |         0.0         |        0.8         |         0.0          |         0.1         |\n",
      "| 37 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 38 |         0.5          |         0.0         |        0.0         |         0.5          |         0.0         |\n",
      "| 39 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 40 |         0.0          |  0.3333333333333333 |        0.0         |  0.6666666666666666  |         0.0         |\n",
      "| 41 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 42 |         0.5          |         0.0         |        0.0         |         0.5          |         0.0         |\n",
      "| 43 |         0.6          |         0.4         |        0.0         |         0.0          |         0.0         |\n",
      "| 44 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 45 |         0.25         |         0.5         |        0.0         |         0.25         |         0.0         |\n",
      "| 46 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 47 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 48 |  0.5384615384615384  |  0.3076923076923077 |        0.0         | 0.15384615384615385  |         0.0         |\n",
      "| 49 |         0.0          |         0.5         |        0.0         |         0.5          |         0.0         |\n",
      "| 50 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 51 |         1.0          |         0.0         |        0.0         |         0.0          |         0.0         |\n",
      "| 52 |  0.8888888888888888  |  0.1111111111111111 |        0.0         |         0.0          |         0.0         |\n",
      "| 53 |  0.3333333333333333  |         0.0         |        0.0         |         0.5          | 0.16666666666666666 |\n",
      "| 54 |         0.5          |         0.5         |        0.0         |         0.0          |         0.0         |\n",
      "| 55 |  0.8333333333333334  |         0.0         |        0.0         | 0.16666666666666666  |         0.0         |\n",
      "| 56 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 57 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 58 |         0.25         |         0.0         |        0.0         |         0.75         |         0.0         |\n",
      "| 59 |          0           |          0          |         0          |          0           |          0          |\n",
      "| 60 |  0.5714285714285714  |         0.0         |        0.0         | 0.42857142857142855  |         0.0         |\n",
      "| 61 |         0.0          |         0.0         |        0.0         |         1.0          |         0.0         |\n",
      "| 62 |         0.2          |         0.1         |        0.0         |         0.6          |         0.1         |\n",
      "+----+----------------------+---------------------+--------------------+----------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #Cadena de texto\n",
    "    #---------------PRIMERA PARTE--------------#\n",
    "    fname='text.htm'#Cambiamos por txt despues de procesar htm------2\n",
    "    articles = get_articles(fname)\n",
    "    #articles = get_text_string(articles)\n",
    "    #articlesWithTags = tagSentencesSpanishTagger(articles)\n",
    "    #serializeObject(articlesWithTags,'articlesTagged.pkl')\n",
    "    topicos = (['gobierno','n'],['política','n'],['internet','n'],['méxico','n'],['mercado','n'])\n",
    "    articlesWithTags = deserializeObject('articlesTagged.pkl')\n",
    "    list_type_articles = []\n",
    "    \n",
    "    for articulo in articlesWithTags:\n",
    "        topics =[]\n",
    "        aux = 0\n",
    "        for topico in topicos:\n",
    "            line = []\n",
    "            count = 0\n",
    "            for word in articulo:\n",
    "                if word == tuple(topico):\n",
    "                    count +=1\n",
    "            aux = aux+count\n",
    "            line.append(topico)\n",
    "            line.append(count)\n",
    "            topics.append(line)\n",
    "        topics2 = []\n",
    "        if aux != 0 :\n",
    "            numbers = [x[1] / aux for x in topics]\n",
    "            for i,topico in enumerate(topicos):\n",
    "                list_a = []\n",
    "                list_a.append(topico)\n",
    "                list_a.append(numbers[i])\n",
    "                topics2.append(list_a)\n",
    "        else:\n",
    "            topics2 = topics\n",
    "        #topics2.sort(key = lambda x: x[1], reverse=True)\n",
    "        list_type_articles.append(topics2)\n",
    "    print(len(list_type_articles))\n",
    "    \n",
    "    x = PrettyTable()\n",
    "    x.field_names = ['N°',topicos[0],topicos[1], topicos[2], topicos[3], topicos[4]]\n",
    "    for i,par in enumerate(list_type_articles):\n",
    "        x.add_row([i,par[0][1],par[1][1],par[2][1],par[3][1],par[4][1]])\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
