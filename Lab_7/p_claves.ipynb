{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.text import Text\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import re\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertimos en un Pk los objetos\n",
    "def serializeObject(obj, fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"wb\") as f:\n",
    "        pickle.dump(obj, f, -1)\n",
    "#desconvertimos los pkl\n",
    "def deserializeObject(fname=\"lemmas2.pkl\"):\n",
    "    with open(fname, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fname):\n",
    "    f=open(fname,encoding='utf-8')\n",
    "    t=f.read()\n",
    "    soup = BeautifulSoup(t,'lxml')\n",
    "    text_string =soup.get_text() #extracción de la cadena\n",
    "\n",
    "    return text_string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagSentencesSpanishTagger(s):\n",
    "    sentences_tagged = []\n",
    "    spanish_tagger = deserializeObject(\"tagger.pkl\")#agarramos nuestro tagger entrenado\n",
    "    tokens = nltk.word_tokenize(s)#Tokenizamos las oraicones\n",
    "    s_tagged = spanish_tagger.tag(tokens)#Taggeamos los tokens\n",
    "    s_tagged = [(it[0].lower(), it[1][0].lower()) for it in s_tagged]#Pasamos a minusculas\n",
    "    sentences_tagged.append(s_tagged)#Agregamos las oraciones taggeadas\n",
    "    return sentences_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(wordsWithTag, language='spanish'):\n",
    "    '''Receives a list of words and returns another list without stop words'''\n",
    "    return [ word for word in wordsWithTag if word[0] not in stopwords.words(language) ]\n",
    "\n",
    "\n",
    "def clearTokens(tokensWithTag):\n",
    "    '''Receives a list of  with tag and returns another list with the same tokens but only with letters'''\n",
    "    result = []\n",
    "    for token in tokensWithTag:\n",
    "        \n",
    "        clearToken = \"\"\n",
    "        for c in token[0]:\n",
    "            if re.match(r'[a-záéíóúñüA-ZÁÉÍÓÚÑÜ]', c):\n",
    "                clearToken += c\n",
    "        if len(clearToken) > 0:\n",
    "            result.append((clearToken, token[1]))\n",
    "    return result\n",
    "\n",
    "def lemmatize(wordsTagged, fname=\"lemmas2.pkl\"):\n",
    "    lemmas = deserializeObject(fname)\n",
    "    wordsLemmatized = []\n",
    "    for word in wordsTagged:\n",
    "        if word in lemmas.keys():#Si la palabra la encuentra en el dic\n",
    "            wordsLemmatized.append((lemmas[word], word[1]))#Guardamos el lemma con su tag y la agregamos a las palabras lematizadas\n",
    "        else:\n",
    "            wordsLemmatized.append(word)#Si no la encuentra guarda la palabra en el vocabulario en el diccionario\n",
    "    return wordsLemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainingNouns(words):\n",
    "    list_nouns = []\n",
    "    for word in words:\n",
    "        if word[1] == 'n':\n",
    "            list_nouns.append(word)\n",
    "    return list_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(N,frecuencia):\n",
    "    if frecuencia != 0:\n",
    "        return np.log( (N+1)/(frecuencia) )\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def tf(frecuencia):\n",
    "    return np.log(1 + frecuencia)\n",
    "\n",
    "def tf_idf(N,frec):\n",
    "    return tf(frec)*idf(N,frec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #Cadena de texto\n",
    "    #---------------PRIMERA PARTE--------------#\n",
    "    #fname='text.htm'#Cambiamos por txt despues de procesar htm------2\n",
    "    #texto = get_text(fname)\n",
    "    #wordsWithTags = tagSentencesSpanishTagger(texto)\n",
    "    #Limpiamos los Tokens y les removemos las stop words\n",
    "    #clearedTokens = removeStopWords(clearTokens(wordsWithTags[0]))\n",
    "    #Lematización de los tokens\n",
    "    #lemmatizedWords = lemmatize(clearedTokens)\n",
    "    #nouns = obtainingNouns(lemmatizedWords)\n",
    "    #serializeObject(nouns,'nouns.pkl')\n",
    "    #-------FIN DE LA PRIMERA PARTE-------#\n",
    "    nouns = deserializeObject('nouns.pkl')\n",
    "    vocabulary = set(nouns)\n",
    "    \n",
    "    list_frecuency = []\n",
    "    for word in vocabulary:\n",
    "        aux = []\n",
    "        count = 0\n",
    "        for noun in nouns:\n",
    "            if word == noun:\n",
    "                count +=1\n",
    "        aux.append(word)\n",
    "        aux.append(count)\n",
    "        list_frecuency.append(aux)\n",
    "    list_frecuency.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    list_tf_idf = []\n",
    "    \n",
    "    for word in list_frecuency:\n",
    "        value = tf_idf(len(vocabulary),word[1])\n",
    "        list_tf_idf.append([word,value])\n",
    "    \n",
    "    list_tf_idf.sort(key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    with open(\"list_frecuency.txt\", \"w\") as f:\n",
    "        for par in list_frecuency:\n",
    "            f.write(F\"{par[0]} {par[1]}\\n\")\n",
    "\n",
    "    with open(\"list_tf_idf.txt\", \"w\") as f:\n",
    "        for par in list_tf_idf:\n",
    "            f.write(F\"{par[0]} {par[1]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
