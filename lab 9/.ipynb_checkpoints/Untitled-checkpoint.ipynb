{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3468)\t0.13211698201404112\n",
      "  (0, 3214)\t0.1694913099699208\n",
      "  (0, 3019)\t0.23643286501244715\n",
      "  (0, 2855)\t0.09132435919480544\n",
      "  (0, 2651)\t0.21018330355686157\n",
      "  (0, 2309)\t0.1266747923875031\n",
      "  (0, 1850)\t0.19306657623623769\n",
      "  (0, 1645)\t0.28387513911105333\n",
      "  (0, 1613)\t0.19667598343327117\n",
      "  (0, 1423)\t0.14869653561689036\n",
      "  (0, 1050)\t0.1913831830029714\n",
      "  (0, 989)\t0.23137601619988216\n",
      "  (0, 968)\t0.20512645474429655\n",
      "  (0, 889)\t0.17658103206223008\n",
      "  (0, 853)\t0.10780990336881303\n",
      "  (0, 789)\t0.19667598343327117\n",
      "  (0, 663)\t0.21602100708798916\n",
      "  (0, 384)\t0.28387513911105333\n",
      "  (0, 242)\t0.28387513911105333\n",
      "  (0, 228)\t0.1913831830029714\n",
      "  (0, 221)\t0.18977144563240358\n",
      "  (0, 171)\t0.19482829444496857\n",
      "  (0, 129)\t0.28387513911105333\n",
      "  (0, 1)\t0.19482829444496857\n",
      "  (1, 3388)\t0.1654276744605963\n",
      "  :\t:\n",
      "  (1323, 3388)\t0.17073749499089552\n",
      "  (1323, 3099)\t0.09349240517105498\n",
      "  (1323, 3043)\t0.13110827205839756\n",
      "  (1323, 3036)\t0.24030236237829375\n",
      "  (1323, 2855)\t0.09671200737472037\n",
      "  (1323, 2541)\t0.21981608591432877\n",
      "  (1323, 2490)\t0.1642197958673209\n",
      "  (1323, 2374)\t0.2103358922139808\n",
      "  (1323, 2296)\t0.15920022239574977\n",
      "  (1323, 2246)\t0.12083481522248796\n",
      "  (1323, 2182)\t0.22555503734639798\n",
      "  (1323, 2082)\t0.24502596221150624\n",
      "  (1323, 1800)\t0.16919483380297565\n",
      "  (1323, 1737)\t0.12694145714346894\n",
      "  (1323, 1563)\t0.14413658054451917\n",
      "  (1323, 1496)\t0.20445647042473955\n",
      "  (1323, 1415)\t0.24502596221150624\n",
      "  (1323, 1217)\t0.22876508978314805\n",
      "  (1323, 1005)\t0.22555503734639798\n",
      "  (1323, 941)\t0.1629047847474716\n",
      "  (1323, 853)\t0.1141701103802062\n",
      "  (1323, 825)\t0.23607695834268702\n",
      "  (1323, 181)\t0.22555503734639798\n",
      "  (1323, 64)\t0.30062225225851413\n",
      "  (1323, 1)\t0.20632212057435104\n",
      "Accuracy of prediction in 0.9712990936555891\n",
      "Confusion matrix:\n",
      " [[499   0]\n",
      " [ 19 144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       499\n",
      "           1       1.00      0.88      0.94       163\n",
      "\n",
      "    accuracy                           0.97       662\n",
      "   macro avg       0.98      0.94      0.96       662\n",
      "weighted avg       0.97      0.97      0.97       662\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import random\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "\n",
    "#Creamos los vectores X y Y definidos en clase\n",
    "def create_vector_x_y():\n",
    "    f=open('SMS_Spam_Corpus_big.txt')\n",
    "    lines=f.readlines()\n",
    "    #print(lines)\n",
    "    #print(len(lines))\n",
    "    X_corpus = [line.lower() for line in lines]\n",
    "    X_corpus = [nltk.word_tokenize(line) for line in X_corpus]\n",
    "    X_texts = [line[:-2] for line in X_corpus]\n",
    "    #print(\"The size of x_texts is %d\"%len(x_texts))\n",
    "\n",
    "    vec_y=[]\n",
    "    for text in X_corpus:\n",
    "        tag = text[-1].strip()\n",
    "        if tag==\"spam\":\n",
    "            vec_y.append(1)\n",
    "        else:\n",
    "            vec_y.append(0)\n",
    "    #Vamos a taggear\n",
    "    X_pos_tagged  = []\n",
    "    for text in X_corpus:\n",
    "        pos_tagged_text = nltk.pos_tag(text)\n",
    "        X_pos_tagged.append(pos_tagged_text)\n",
    "    \n",
    "    #print(\"\\nText Post tagged \\n\")\n",
    "    #print(X_pos_tagged[500])\n",
    "    \n",
    "    #Vamos a lematizar\n",
    "    from nltk import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X_lemmatized = []\n",
    "    for text in X_pos_tagged:\n",
    "        lemmas = []\n",
    "        for token in text:\n",
    "            str1 = token[0]\n",
    "            str2 = token[1].lower()\n",
    "            try:\n",
    "                str2 = str2[0]\n",
    "                lemma = lemmatizer.lematize(str1,str2)\n",
    "                lemmas.append(lemma.lower())\n",
    "            except:\n",
    "                lemmas.append(str1.lower())\n",
    "        lemmas_string=' '.join(lemmas)\n",
    "        X_lemmatized.append(lemmas_string)\n",
    "    return X_lemmatized,vec_y\n",
    "                \n",
    "\n",
    "def classify_sklearn(X,y):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    count_vect=CountVectorizer()\n",
    "    X_counts=count_vect.fit_transform(X)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    tfidf_transformer=TfidfTransformer()\n",
    "    X_tfidf=tfidf_transformer.fit_transform(X_counts)\n",
    "    \n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=42)\n",
    "    \n",
    "    from mord import LogisticIT\n",
    "    clf = LogisticIT()\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_train = np.array(y_train)\n",
    "    X_train = np.array(X_train)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    print('Accuracy of prediction in', clf.score(X_test,y_test))\n",
    "    print('Confusion matrix:\\n',confusion_matrix(y_test,y_pred))\n",
    "    print(metrics.classification_report(y_test,y_pred))\n",
    "    \n",
    "\n",
    "               \n",
    "if __name__=='__main__':\n",
    "    X,y=create_vector_x_y()\n",
    "    classify_sklearn(X,y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
